@article{Azar2007,
abstract = {In this paper, we investigate several means of visualizing both ambient and speech sounds and present a fusion of different visualization displays into one program package that would help provide the hearing impaired with a means to an enhanced awareness of their surroundings. The ideas investigated were implemented in software, and the program was evaluated by means of a survey conducted in a school for the deaf.},
author = {Azar, Jimmy and Saleh, H and Al-Alaoui, M},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Azar, Saleh, Al-Alaoui - 2007 - Sound visualization for the hearing impaired.pdf:pdf},
journal = {International Journal of Emerging \ldots},
number = {1},
title = {{Sound visualization for the hearing impaired}},
url = {http://webfea.fea.aub.edu.lb/dsaf/Publications/IJET.pdf http://www.editlib.org/p/45106},
volume = {2},
year = {2007}
}
@article{Bartram2001,
abstract = {Simple motion has great potential for visually encoding information but there are as yet few experimentally validated guidelines for its use. Two studies were carried out to look at how efficiently simple motion cues were detected and how distracting they were in different task contexts. The results show that motion outperforms static representations and identify certain types of motions which are more distracting and irritating than others.},
author = {Bartram, L and Ware, C and Calvert, T},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/Moving Icons - Detection And Distraction.pdf:pdf},
journal = {Proceedings of the IFIP TC. 13 \ldots},
title = {{Moving icons: Detection and distraction}},
url = {http://books.google.com/books?hl=en\&lr=\&id=LoR\_qZGX8IgC\&oi=fnd\&pg=PA157\&dq=Moving+Icons+-+Detection+And+Distraction\&ots=PdMw3eRjMM\&sig=zqETBxKDFxxVfCJ9xrbC1gqgiSM},
year = {2001}
}
@article{Bavelier2000a,
abstract = {We compared normally hearing individuals and congenitally deaf individuals as they monitored moving stimuli either in the periphery or in the center of the visual field. When participants monitored the peripheral visual field, greater recruitment (as measured by functional magnetic resonance imaging) of the motion-selective area MT/MST was observed in deaf than in hearing individuals, whereas the two groups were comparable when attending to the central visual field. This finding indicates an enhancement of visual attention to peripheral visual space in deaf individuals. Structural equation modeling was used to further characterize the nature of this plastic change in the deaf. The effective connectivity between MT/MST and the posterior parietal cortex was stronger in deaf than in hearing individuals during peripheral but not central attention. Thus, enhanced peripheral attention to moving stimuli in the deaf may be me- diated by alterations of the connectivity between MT/MST and the parietal cortex, one of the primary centers for spatial rep- resentation and attention.},
author = {Bavelier, D and Tomann, A and Hutton, C},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bavelier, Tomann, Hutton - 2000 - Visual attention to the periphery is enhanced in congenitally deaf individuals.pdf:pdf},
journal = {The Journal of \ldots},
title = {{Visual attention to the periphery is enhanced in congenitally deaf individuals.}},
url = {http://doi.apa.org/psycinfo/2001-03295-001},
year = {2000}
}
@article{Bavelier2000,
abstract = {We compared normally hearing individuals and congenitally deaf individuals as they monitored moving stimuli either in the periphery or in the center of the visual field. When participants monitored the peripheral visual field, greater recruitment (as measured by functional magnetic resonance imaging) of the motion-selective area MT/MST was observed in deaf than in hearing individuals, whereas the two groups were comparable when attending to the central visual field. This finding indicates an enhancement of visual attention to peripheral visual space in deaf individuals. Structural equation modeling was used to further characterize the nature of this plastic change in the deaf. The effective connectivity between MT/MST and the posterior parietal cortex was stronger in deaf than in hearing individuals during peripheral but not central attention. Thus, enhanced peripheral attention to moving stimuli in the deaf may be me- diated by alterations of the connectivity between MT/MST and the parietal cortex, one of the primary centers for spatial rep- resentation and attention.},
author = {Bavelier, D and Tomann, A and Hutton, C and Mitchell, T and Corina, D and Liu, G and Neville, H},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bavelier, Tomann, Hutton - 2000 - Visual attention to the periphery is enhanced in congenitally deaf individuals.pdf:pdf},
journal = {The Journal of Neuroscience},
keywords = {alternating blocks static,deafness,dots flow fields,fmri,known efficiently recruit,motion,motion flow fields,moving dots,mst v5,mt,plasticity,structural equation modeling,visual attention},
number = {RC93},
pages = {1--6},
title = {{Visual Attention to the Periphery Is Enhanced in Congenitally Deaf Individuals}},
volume = {20},
year = {2000}
}
@misc{Borg,
abstract = {A sound localization aid based on eyeglasses with three microphones and four vibrators was tested in a sound-treated acoustic test room and in an ordinary office. A digital signal-processing algorithm provided a determination of the source angle, which was transformed into eight vibrator codes each corresponding to a 45 degrees sector. The instrument was tested on nine deaf and three deaf-blind individuals. The results show an average hit rate of about 80\% in a sound-treated room with 100\% for the front 135 degrees sector. The results in a realistic communication situation in an ordinary office room were 70\% correct based on single presentations and 95\% correct when more realistic criteria for an adequate reaction were used. Ten of the twelve subjects were interested in participating in field tests using a planned miniaturized version.},
author = {Borg, E and R\"{o}nnberg, J and Neovius, L},
booktitle = {Journal of rehabilitation research and development},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/Vibratory coded directional analysis evaluation of a three-microphone four-vibrator DSP system.pdf:pdf},
issn = {0748-7711},
keywords = {Adult,Aged,Aged, 80 and over,Blindness,Deafness,Deafness: rehabilitation,Humans,Middle Aged,Sensory Aids,Signal Processing, Computer-Assisted,Sound Localization,Vibration},
number = {2},
pages = {257--63},
pmid = {11392658},
title = {{Vibratory-coded directional analysis: evaluation of a three-microphone/four-vibrator DSP system.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11392658},
volume = {38}
}
@article{Bosman2003,
author = {Bosman, S and Groenendaal, B},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/GentleGuide An exploration of haptic output for indoors pedestrain guidance.pdf:pdf},
journal = {\ldots interaction with mobile \ldots},
title = {{Gentleguide: An exploration of haptic output for indoors pedestrian guidance}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-45233-1\_28},
year = {2003}
}
@article{Cavender2008a,
abstract = {The recommendations in this article reflect current thinking on language for writing in the academic accessibility community. Certain words or phrases can (intentionally or unintentionally) reflect bias or negative, disparaging, or patronizing attitudes toward people with disabilities and in fact any identifiable group of people. Because language can convey these things, it can influence our impressions, attitudes, and even actions. Choosing language that is neutral, accurate, and represents the preference of the groups to which it refers can convey respect and integrity.},
author = {Cavender, Anna and Trewin, Shari and Hanson, Vicki},
doi = {10.1145/1452562.1452565},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cavender, Trewin, Hanson - 2008 - General Writing Guidelines for Technology and People with Disabilities(2).pdf:pdf},
journal = {ACM SIGACCESS Accessibility and Computing},
number = {92},
pages = {17--22},
title = {{General Writing Guidelines for Technology and People with Disabilities}},
url = {http://dl.acm.org/citation.cfm?id=1452562.1452565},
year = {2008}
}
@article{Codina2011,
author = {Codina, C and Pascalis, O and Mody, C and Toomey, P},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/Visual Advantage in Deaf Adults Linked to Retinal.pdf:pdf},
journal = {PloS one},
title = {{Visual advantage in deaf adults linked to retinal changes}},
url = {http://dx.plos.org/10.1371/journal.pone.0020417},
year = {2011}
}
@misc{Codina2011a,
abstract = {This study investigated peripheral vision (at least 30° eccentric to fixation) development in profoundly deaf children without cochlear implantation, and compared this to age-matched hearing controls as well as to deaf and hearing adult data. Deaf and hearing children between the ages of 5 and 15 years were assessed using a new, specifically paediatric designed method of static perimetry. The deaf group (N = 25) were 14 females and 11 males, mean age 9.92 years (range 5-15 years). The hearing group (N = 64) were 34 females, 30 males, mean age 9.13 years (range 5-15 years). All participants had good visual acuity in both eyes (< 0.200 LogMAR). Accuracy of detection and reaction time to briefly presented LED stimuli of three light intensities, at eccentricities between 30° and 85° were measured while fixation was maintained to a central target. The study found reduced peripheral vision in deaf children between 5 and 10 years of age. Deaf children (aged 5-10 years) showed slower reaction times to all stimuli and reduced ability to detect and accurately report dim stimuli in the far periphery. Deaf children performed equally to hearing children aged 11-12 years. Deaf adolescents aged 13-15 years demonstrated faster reaction times to all peripheral stimuli in comparison to hearing controls. Adolescent results were consistent with deaf and hearing adult performances wherein deaf adults also showed significantly faster reaction times than hearing controls. Peripheral vision performance on this task was found to reach adult-like levels of maturity in deaf and hearing children, both in reaction time and accuracy of detection at the age of 11-12 years.},
author = {Codina, Charlotte and Buckley, David and Port, Michael and Pascalis, Olivier},
booktitle = {Developmental science},
doi = {10.1111/j.1467-7687.2010.01017.x},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/Deaf and Hearing Children A Comparison of Peripheral Vision Development.pdf:pdf},
issn = {1467-7687},
keywords = {Adolescent,Child,Child, Preschool,Deafness,Female,Hearing,Humans,Male,Reaction Time,Reaction Time: physiology,Vision, Ocular,Vision, Ocular: physiology,Visual Acuity,Visual Acuity: physiology,Visual Field Tests},
month = jul,
number = {4},
pages = {725--37},
pmid = {21676093},
title = {{Deaf and hearing children: a comparison of peripheral vision development.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21676093},
volume = {14},
year = {2011}
}
@article{Dodgson1983,
abstract = {The development and evaluation of a sensory electrical substitution aid for the profoundly deaf is described. The aid is a smal wrist-worn device which converts sound into an electrical stimulus at the wrist. The initial aim was to give profoundly deaf people useful information from ambient sounds. Evaluation of the Mark 1 aid on 15 hearing and five profoundly deaf people indicated that such a device might be of use to some deaf people. Investigation of the sensitivity of the peripheral nervous system to electrical pulse stimulation showed that information could be transmitted in a more efficient manner so a Mark 2 aid was developed which incorporated circuitry to transmit pitch information, potentially useful in lipreading. The Mark 2 aid was shown to give improved results over the Mark 1 aid in transmitting voice intonation, but gave no help in an isolated word lipreading test. A portable Mark 2 aid was developed to enable clinical trials to be carried out on profoundly deaf people.},
author = {Dodgson, GS and Brown, BH},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dodgson, Brown - 1983 - Electrical stimulation at the wrist as an aid for the profoundly deaf.pdf:pdf},
journal = {Clinical Physics and \ldots},
keywords = {auditory system,electrostimulation,hearing impairment,human,joint,nerve,nervous system,peripheral nervous system,therapy,wrist},
number = {4},
pages = {403--416},
title = {{Electrical stimulation at the wrist as an aid for the profoundly deaf}},
url = {http://ovidsp.ovid.com/ovidweb.cgi?T=JS\&CSC=Y\&NEWS=N\&PAGE=fulltext\&D=emed1a\&AN=1984067976 http://iopscience.iop.org/0143-0815/4/4/005},
volume = {4},
year = {1983}
}
@article{Dye2009,
abstract = {BACKGROUND: Early deafness leads to enhanced attention in the visual periphery. Yet, whether this enhancement confers advantages in everyday life remains unknown, as deaf individuals have been shown to be more distracted by irrelevant information in the periphery than their hearing peers. Here, we show that, in a complex attentional task, a performance advantage results for deaf individuals. METHODOLOGY/PRINCIPAL FINDINGS: We employed the Useful Field of View (UFOV) which requires central target identification concurrent with peripheral target localization in the presence of distractors - a divided, selective attention task. First, the comparison of deaf and hearing adults with or without sign language skills establishes that deafness and not sign language use drives UFOV enhancement. Second, UFOV performance was enhanced in deaf children, but only after 11 years of age. CONCLUSIONS/SIGNIFICANCE: This work demonstrates that, following early auditory deprivation, visual attention resources toward the periphery slowly get augmented to eventually result in a clear behavioral advantage by pre-adolescence on a selective visual attention task.},
author = {Dye, Matthew W G and Hauser, Peter C and Bavelier, Daphne},
editor = {Baker, Chris I},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dye, Hauser, Bavelier - 2009 - Is Visual Selective Attention in Deaf Individuals Enhanced or Deficient The Case of the Useful Field of View(2).pdf:pdf},
institution = {Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York, USA. mdye@bcs.rochester.edu},
journal = {PLoS ONE},
keywords = {adolescent,adult,attention,attention physiology,child,female,hearing impaired persons,humans,male,sign language,task performance analysis,visual fields,visual fields physiology,visual perception,visual perception physiology},
number = {5},
pages = {e5640},
publisher = {Public Library of Science},
title = {{Is Visual Selective Attention in Deaf Individuals Enhanced or Deficient? The Case of the Useful Field of View}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2680667\&tool=pmcentrez\&rendertype=abstract},
volume = {4},
year = {2009}
}
@article{Hahn2005,
abstract = {In this paper we present an assistive system for hearing-impaired people that consists of a wearable microphone array and an Augmented Reality (AR) system. This system helps the user in communication situations, where many speakers or sources of background noise are present. In order to restore the ``cocktail party'' effect multiple microphones are used to estimate the position of individual sound sources. In order to allow the user to interact in complex situations with many speakers, an algorithm for estimating the user's attention is developed. This algorithm determines the sound sources, which are in the user's focus of attention. It allows the system to discard irrelevant information and enables the user to focus on certain aspects of the surroundings. Based on the user's hearing impairment, the perception of the speaker in the focus of attention can be enhanced, e.g. by amplification or using a speech-to-text conversion. A prototype has been built for evaluating this approach. Currently the prototype is able to locate sound beacons in three-dimensional space, to perform a simple focus estimation, and to present floating captions in the Augmented Reality. The prototype uses an intentionally simple user interface, in order to minimize distractions.},
author = {Hahn, Daniel and Beutler, Frederik and Hanebeck, UD},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hahn, Beutler, Hanebeck - 2005 - Visual scene augmentation for enhanced human perception.pdf:pdf},
journal = {ICINCO},
pages = {146--153},
publisher = {Citeseer},
title = {{Visual scene augmentation for enhanced human perception.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.166\&amp;rep=rep1\&amp;type=pdf http://i81pc23.itec.uni-karlsruhe.de/Publikationen/ICINCO05\_Hahn.pdf},
volume = {2},
year = {2005}
}
@inproceedings{Ho-Ching2003,
abstract = {We developed two visual displays for providing awareness of environmental audio to deaf individuals. Based on fieldwork with deaf and hearing participants, we focused on supporting awareness of non-speech audio sounds such as ringing phones and knocking in a work environment. Unlike past work, our designs support both monitoring and notification of sounds, support discovery of new sounds, and do not require a priori knowledge of sounds to be detected. Our Spectrograph design shows pitch and amplitude, while our Positional Ripples design shows amplitude and location of sounds. A controlled experiment involving deaf participants found neither display to be significantly distracting. However, users preferred the Positional Ripples display and found that display easier to monitor (notification sounds were detected with 90\% success in a laboratory setting). The Spectrograph display also supported successful detection in most cases, and was well received when deployed in the field.},
author = {Ho-Ching, F Wai-ling and Mankoff, Jennifer and Landay, James A},
booktitle = {New Horizons},
doi = {10.1145/642611.642641},
isbn = {1581136307},
keywords = {13,6,asl,awareness non speech,background speech,defined here include,language,little research has addressed,sound,translation american sign},
number = {5},
pages = {161--168},
publisher = {ACM Press},
title = {{Can you see what i hear?: the design and evaluation of a peripheral sound display for the deaf}},
url = {http://www.cs.cmu.edu/~io/publications/old-pubs/469-ho-ching.pdf},
year = {2003}
}
@article{Kim2013,
author = {Kim, KW and Choi, JW and Kim, YH},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Choi, Kim - 2013 - An assistive device for direction estimation of a sound source.pdf:pdf},
journal = {Assistive Technology},
title = {{An assistive device for direction estimation of a sound source}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10400435.2013.768718},
year = {2013}
}
@article{Matthews2006a,
abstract = {Sounds such as co-workers chatting nearby or a dripping faucet help us maintain awareness of and respond to our surroundings. Without a tool that communicates ambient sounds in a non-auditory manner, maintaining this awareness is diﬃcult for people who are deaf. We present an iterative investigation of peripheral, visual displays of ambient sounds. Our major contributions are: (1) a rich understanding of what ambient sounds are useful to people who are deaf, (2) a set of visual and functional requirements for a peripheral sound display, based on feedback from people who are deaf, (3) labbased evaluations investigating the characteristics of four prototypes, and (4) a set of design guidelines for successful ambient audio displays, based on a comparison of four implemented prototypes and user feedback. Our work provides valuable information about the sound awareness needs of the deaf and can help to inform further design of such applications.},
author = {Matthews, Tara and Fong, Janette and Ho-Ching, F Wai-Ling and Mankoff, Jennifer},
doi = {10.1080/01449290600636488},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthews et al. - 2006 - Evaluating non-speech sound visualizations for the deaf.pdf:pdf},
isbn = {0144929060063},
issn = {0144929X},
journal = {Behaviour Information Technology},
number = {4},
pages = {333--351},
publisher = {Citeseer},
title = {{Evaluating non-speech sound visualizations for the deaf}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01449290600636488},
volume = {25},
year = {2006}
}
@article{Matthews2005,
abstract = {Sounds constantly occur around us, keeping us aware of our surroundings. People who are deaf have difficulty maintaining an awareness of these ambient sounds. We present an investigation of peripheral, visual displays to help people who are deaf maintain an awareness of sounds in the environment. Our contribution is twofold. First, we present a set of visual design preferences and functional requirements for peripheral visualizations of non-speech audio that will help improve future applications. Visual design preferences include ease of interpretation, glance-ability, and appropriate distractions. Functional requirements include the ability to identify what sound occurred, view a history of displayed sounds, customize the information that is shown, and determine the accuracy of displayed information. Second, we designed, implemented, and evaluated two fully functioning prototypes that embody these preferences and requirements, serving as examples for future designers and furthering progress toward understanding how to best provide peripheral audio awareness for the deaf.},
author = {Matthews, Tara and Fong, Janette and Mankoff, Jennifer},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthews, Fong, Mankoff - 2005 - Visualizing non-speech sounds for the deaf.pdf:pdf},
journal = {Proceedings of the 7th international \ldots},
keywords = {deaf,peripheral display,sound visualization},
number = {23},
pages = {52},
publisher = {ACM Press},
title = {{Visualizing non-speech sounds for the deaf}},
url = {http://portal.acm.org/citation.cfm?doid=1090785.1090797 http://dl.acm.org/citation.cfm?id=1090797},
volume = {25},
year = {2005}
}
@article{O'Brien1985,
author = {O'Brien, RG and Kaiser, MK},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/MANOVA Method for Analyzing Repeated Measures Designs an Extensive Primer..pdf:pdf},
journal = {Psychological bulletin},
title = {{MANOVA Method for Analyzing Repeated Measures Designs an Extensive Primer}},
url = {http://psycnet.apa.org/journals/bul/97/2/316/},
year = {1985}
}
@article{Shen2012,
abstract = {In recent years, Augmented Reality(AR) has been widely used in various research. AR augments human sensation and offers information to support users in daily life. In this research, we propose an AR system that recognizes sound sources to augment humans vision. In our system, a sound source and its position are detected by acoustic processing. The system notifies a user through different visual markers, which are allocated on different kinds of sound sources. Our system detects different kinds of sound sources in daily life and notifies user of the information and the position of sound source. Using our system, the user can find out which object is making sound in the environment without hearing it.},
author = {Shen, Ruiwei and Terada, Tsutomu and Tsukamoto, Masahiko},
doi = {10.1145/2428955.2428979},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen, Terada, Tsukamoto - 2012 - A System for Visualizing Sound Source using Augmented Reality.pdf:pdf},
journal = {Proceedings of the 10th International Conference on Advances in Mobile Computing Multimedia MoMM 12},
keywords = {augmented reality,hear impaired,sound recognition,wearable},
pages = {97},
title = {{A System for Visualizing Sound Source using Augmented Reality}},
url = {http://dl.acm.org/citation.cfm?doid=2428955.2428979},
year = {2012}
}
@article{Tessendorf2011,
author = {Tessendorf, B and Roggen, D},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/Bilateral Vibrotactile Feedback Patterns for Accurate Lateralization in Hearing Instrument Body Area Networks.pdf:pdf},
journal = {\ldots on Body Area Networks},
title = {{Bilateral vibrotactile feedback patterns for accurate lateralization in hearing instrument body area networks}},
url = {http://dl.acm.org/citation.cfm?id=2318802},
year = {2011}
}
@article{Tomitsch2007,
author = {Tomitsch, M and Grechenig, T},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/Design Implications for a Ubiquitous Ambient Sound Display for the Deaf.pdf:pdf},
journal = {CVHI},
title = {{Design Implications for a Ubiquitous Ambient Sound Display for the Deaf.}},
url = {http://137.226.34.227/Publications/CEUR-WS/Vol-415/paper24.pdf},
year = {2007}
}
@article{Tsukada2004,
author = {Tsukada, K and Yasumura, M},
file = {:C$\backslash$:/Users/Johnny/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsukada, Yasumura - 2004 - Activebelt Belt-type wearable tactile display for directional navigation.pdf:pdf},
journal = {UbiComp 2004: Ubiquitous Computing},
title = {{Activebelt: Belt-type wearable tactile display for directional navigation}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-30119-6\_23},
year = {2004}
}
@article{Wallach1939,
author = {Wallach, H},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/On Sound Localization.pdf:pdf},
journal = {The Journal of the Acoustical Society of America},
title = {{On sound localization}},
url = {http://pdfserv.aip.org/JASMAN/vol\_10/iss\_4/270\_1.pdf},
year = {1939}
}
@article{Weisenberger1987,
author = {Weisenberger, J and Heidbreder, A and Miller, J},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/Development and preliminary evaluation of an earmold sound-to-tactile aid for the hearing-impaired.pdf:pdf},
journal = {J Rehabil Res Dev},
title = {{Development and preliminary evaluation of an earmold sound-to-tactile aid for the hearing-impaired}},
url = {http://www.rehab.research.va.gov/jour/87/24/2/pdf/weisenberger.pdf},
year = {1987}
}
@article{Tan1997,
author = {Tan, HZ and Pentland, A},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/Tactual. Disptays for Wearabte Computing.pdf:pdf},
journal = {Personal Technologies},
title = {{Tactual displays for wearable computing}},
url = {http://link.springer.com/article/10.1007/BF01682025},
year = {1997}
}
@article{Raj2000,
author = {Raj, a. K. and Kass, S. J. and Perry, J. F.},
doi = {10.1177/154193120004400148},
file = {:D$\backslash$:/Dropbox/碩論/VISSO Reference Papers/Vibrotactile displays for improving spatial awareness.pdf:pdf},
issn = {1071-1813},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
month = jul,
number = {1},
pages = {181--184},
title = {{Vibrotactile Displays for Improving Spatial Awareness}},
url = {http://pro.sagepub.com/lookup/doi/10.1177/154193120004400148},
volume = {44},
year = {2000}
}
@misc{Wilcox2012,
author = {Wilcox, Dominic},
title = {{No Place Like Home, GPS shoes}},
note = {http://dominicwilcox.com/portfolio/gpsshoe/}
}
@misc{CyberGlove,
title = {{CyberGlove and CyberTouch}},
author = {CyberGlove Systems LLC},
note = {http://cyberglovesystems.com/all-products}
}